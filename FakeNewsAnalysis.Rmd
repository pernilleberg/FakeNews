---
title: "Analysis; The Spread of Fake News"
author: "Pernille Berg Lassen"
date: "28 apr 2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

Analyzing the spread of Fake News in a Social Network.

Outline:
  1. Short description of main questions and hypotheses 
  2. Short description of network used to simulate the spread - it's structure, which parameters etc.
  3. Data cleaning (Probably necessary)
  4. Models used for analysis: Bayesian (brms or map2stan?)
  5. Assessing model quality
  6. Results (Pretty plots and some estimates)
  

Main Questions and Hyptheses 

This paper set out to investigate how different network characteristics influence the spread of Fake News in a Social Network. Mainly, we are interested in which parameters must be present (and to what extent) to limit the spread of Fake News stories in a thightly interconnected network

  We will explore this question from two different persepectives: 1) an 'assigned-Authority' perspective, where the modulators/modifiers (named 'WatchDogs') are randomly distributed throughtout the network, and have a fixed probability of detecting fake news, and 2) a 'leave-it-to-the-population' perspective, where all agents/nodes within the network has a probability of detecting fake news, and afterwards punish the agent from which they received the Fake News by isolating it from the network (equavilent to ban someone from the Social Network platform).
  
  In essense, these perspetives represent two different approaches to try and deal with fake news on Social Media. The first persepctive, 'assigned-Authority', represent an intervention where the Network Providers (e.g. Facebook Staff) assign the task of flagging/stopping fake news to different individuals within the network. The second persepctive, 'leave-it-to-the-population', represent the idea of having the network regulate it self, leaving it to the population (its users) to detect fake news.  
  Furthermore, social media is hypothezied to be an optimal environment for Fake News due to its structure. A social network is characterized by a high clustering coeffecient (i.e. users are thightly connected in 'cliques' or clusters). Additionally, social media is charaterized by a community structure where links are dense within communities but sparse between them.
  
  

The network:

Using the NetLogo model 'Virus on a Network' as template. 
A small world network
Specify a number of WD as the beginning (and specify their ability)
Calcualting the measures - number of communities, clustering coeffecient, average-path-length, etc. 
  

  Clustering coefficient = a measure of how likely it is that two connected nodes are part of the some larger, highly connected group of nodes (A probability measure, between 0-1).


```{r}
library(pacman)
p_load(rethinking,brms,brmstools,ggplot2,caret)


#Reading in Data and cleaning it
FN_df = read.csv("FakeNewsData3test1.csv", sep = ";")
FN_df$ID = 1
FN_df2 = read.csv("FakeNewsData3test2.csv", sep = ";")
FN_df2$ID = 2

dataset<-rbind(FN_df, FN_df2)

# Change column names
colnames(dataset) = c("runNumber", "nodesAmount", "outbreakSize", "numberofWD", "recovery", "tick", "turtlesAmount", "clusteringCoef", "pathLength", "naivesAmount", "spreadersAmount", "educatedAmount", "untouchedAmount", "fileID")

dataset$turtlesAmount = NULL

# blue ones = naives
# red = spreaders
# grey = educated
# yellow = untouched

# Order based on file ID so that the data from the same simulation comes together
dataset$runNumber = as.numeric(dataset$runNumber)
dat <- dataset[order(dataset$fileID),]

# Scaled dataframe - in case we want to do that
#preObj <- preProcess(dat[, c(-1,-14)], method=c("center", "scale"))
#datscaled <- predict(preObj, dat)
# Should not need this because:
#These variables have zero variances: nodesAmount, recovery, turtlesAmount

```

```{r}
#PLaying around with plots - some visualizations about development of agents over time

#Amount of spreaders over time when outbreak size is maximum
temp = subset(dataset, outbreakSize == "188")
temp$numberofWD = as.factor(temp$numberofWD)

ggplot(temp,aes(tick,spreadersAmount, color = numberofWD))+
  geom_point()+
  ggtitle("Outbreak Size = 188 (Max)")+
  theme_classic()

#Amount of spreaders over time when outbreak size is minimum
temp = subset(dataset, outbreakSize == "1")
temp$numberofWD = as.factor(temp$numberofWD)

ggplot(temp, aes(tick,spreadersAmount, color = numberofWD))+
  geom_point()+
  ggtitle("Outbreak Size = 1 (Min)")+
  theme_classic()

#Amount of spreaders over time when number of wd is minimum
temp = subset(dataset, numberofWD == "10")
temp$outbreakSize = as.factor(temp$outbreakSize)

ggplot(temp, aes(tick,spreadersAmount, color = outbreakSize))+
  geom_point()+
  ggtitle("Number of WD = 10 (Min)")+
  theme_classic()

#Amount of spreaders over time when number of wd is maximum
temp = subset(dataset, numberofWD == "50")
temp$outbreakSize = as.factor(temp$outbreakSize)

ggplot(temp, aes(tick,spreadersAmount, color = outbreakSize))+
  geom_point()+
  ggtitle("Number of WD = 50 (Max)")+
  theme_classic()

temp = dataset
temp$outbreakSize = as.factor(temp$outbreakSize)

#Agent development over time
ggplot(temp, aes(tick, untouchedAmount, color = outbreakSize))+
  geom_smooth()+
  facet_wrap(~outbreakSize)+
  theme_classic()

ggplot(temp, aes(tick, educatedAmount, color = outbreakSize))+
  geom_smooth()+
  facet_wrap(~outbreakSize)+
  theme_classic()

ggplot(temp, aes(tick, spreadersAmount, color = outbreakSize))+
  geom_smooth()+
  facet_wrap(~outbreakSize)+
  theme_classic()

ggplot(temp, aes(tick, naivesAmount, color = outbreakSize))+
  geom_smooth()+
  facet_wrap(~outbreakSize)+
  theme_classic()
```


```{r}
#Adding new columns:
dataset$Ratio_WD = dataset$nodesAmount/dataset$numberofWD #we expect an interaction between number of nodes and number of WD - using ratio in models instead 
dataset$ID_new = as.factor(paste(dataset$fileID, dataset$runNumber, sep = "_")) #making a new ID column which takes into account which fileID (some runNumbers are the same - can't use that as repeated measure)

library(dplyr)
df <- arrange(dataset, ID_new)

#Summarize by so we know the ticks it took to complete each sim - our outcome
maxTick = group_by(df,ID_new) %>%
  summarize(maxTick = max(tick))
new_df=merge(df,maxTick, all = T)

m1_formula <- bf(maxTick ~ outbreakSize + Ratio_WD + (1|ID_new)) #The last bit is how you add repeated measures
#We need to consider: Random effects?


get_prior(m1_formula,new_df) #Asking the model which priors it recommend

prior = c(prior(student_t(3,10,10), class = Intercept),
          prior(normal(0,1), class = b, coef = outbreakSize), #what to do when categorical?
          prior(normal(0,1), class = b, coef = Ratio_WD), #what do to when categorical?
          prior(student_t(3,0,10), class = sigma))

m1 <- brm(m1_formula,
          family = student_t(), #We assume our likelihood function to be long-tailed (left skewed)
          prior = prior, #our list of pre-defined priors
          data = new_df,
          iter = 2000,
          cores = 2,
          chain = 2)

summary(m1)
plot(m1)

m2_formula <- bf(maxTick ~ clusteringCoef + pathLength + outbreakSize + Ratio_WD + (1|ID_new)) #The last bit is how you add repeated measures?
#We need to consider: Random effects?
#Remember: when comparing the models, this model should be punished because it's adding all them parameters 


get_prior(m2_formula,new_df) #Asking the model which priors it recommend

prior = c(prior(normal(0,1), class = Intercept),
          prior(normal(0,1), class = b, coef = outbreakSize),
          prior(normal(0,1), class = b, coef = Ratio_WD),
          prior(cauchy(0,2), class = sigma))

m2 <- brm(m2_formula,
          family = gaussian(), #We assume our likelihood function to be normally distributed
          prior = prior, #our list of pre-defined priors
          data = new_df,
          iter = 2000,
          cores = 2,
          chain = 2)

summary(m2)
plot(m2)
```
